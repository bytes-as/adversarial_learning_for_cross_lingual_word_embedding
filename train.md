
### Basic Idea
Mapping of two sets of pre-trained monolingual word embeddings
was first proposed by '__some great guy I don't want to know__'. They uses a small dictionary of '__n__' pairs of words __{__(w<sub>s<sub>1</sub></sub>, w<sub>t<sub>1</sub></sub>), (w<sub>s<sub>2</sub></sub>, w<sub>t<sub>2</sub></sub>),... ,(w<sub>s<sub>n</sub></sub>, w<sub>t<sub>n</sub></sub>)__}__ obtained from Google Translate to learn a transformation matrix __W__ that projects the embeddings vsi of the source language words __w<sub>s<sub>i</sub></sub>__ onto the embeddings __v<sub>t<sub>i</sub></sub>__ of their transaltion words wti in the target language:


$\underset{c\in C}{\operatorname{argmax}}$

some mathematical equation



